{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf8')\n",
    "\n",
    "import twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use the training data to test the model\n",
    "\n",
    "def createTestData(TestFile):\n",
    "    import csv\n",
    "    testData=[]\n",
    "    with open(TestFile,'r') as csvfile:\n",
    "        lineReader = csv.reader(csvfile,delimiter=',',quotechar=\"\\\"\")\n",
    "        for row in lineReader:\n",
    "            testData.append({\"text\":row[0],\"label\":None})\n",
    "    return testData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TestFile=\"C:\\Users\\songsu\\Desktop\\Sentiment Analysis\\TestFile.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# testData=createTestData(TestFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# testData[0:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createTrainingCorpus(corpusFile):\n",
    "    import csv\n",
    "    trainingData=[]\n",
    "    with open(corpusFile,'r') as csvfile:\n",
    "        lineReader = csv.reader(csvfile,delimiter=',',quotechar=\"\\\"\")\n",
    "        for row in lineReader:\n",
    "            trainingData.append({\"text\":row[0],\"label\":row[1]})\n",
    "    return trainingData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpusFile=\"C:\\Users\\songsu\\Desktop\\Training Data\\Compiled.csv\"\n",
    "trainingData=createTrainingCorpus(corpusFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# trainingData[0:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpusFileSVM=\"C:\\Users\\songsu\\Desktop\\Training Data\\Compiled_SVM.csv\"\n",
    "trainingDataSVM=createTrainingCorpus(corpusFileSVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'positive',\n",
       "  'text': '\\xef\\xbb\\xbfNow all @Apple has to do is get swype on the iphone and it will be crack. Iphone that is'},\n",
       " {'label': 'positive',\n",
       "  'text': \"Hilarious @youtube video - guy does a duet with @apple 's Siri. Pretty much sums up the love affair! http://t.co/8ExbnQjY\"},\n",
       " {'label': 'positive',\n",
       "  'text': '@RIM you made it too easy for me to switch to @Apple iPhone. See ya!'},\n",
       " {'label': 'positive',\n",
       "  'text': 'The 16 strangest things Siri has said so far. I am SOOO glad that @Apple gave Siri a sense of humor! http://t.co/TWAeUDBp via @HappyPlace'},\n",
       " {'label': 'positive',\n",
       "  'text': 'Great up close & personal event @Apple tonight in Regent St store!'},\n",
       " {'label': 'positive',\n",
       "  'text': 'From which companies do you experience the best customer service aside from @zappos and @apple?'},\n",
       " {'label': 'positive',\n",
       "  'text': 'Just apply for a job at @Apple, hope they call me lol'},\n",
       " {'label': 'positive',\n",
       "  'text': 'Lmao I think @apple is onto something magical! I am DYING!!! haha. Siri suggested where to find whores and where to hide a body lolol'},\n",
       " {'label': 'positive',\n",
       "  'text': \"RT @PhillipRowntree: Just registered as an @apple developer... Here's hoping I can actually do it... Any help, greatly appreciated!\"}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainingDataSVM[0:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from string import punctuation \n",
    "from nltk.corpus import stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PreProcessTweets:\n",
    "    def __init__(self):\n",
    "        self._stopwords=set(stopwords.words('english')+list(punctuation)+['AT_USER','URL'])\n",
    "        \n",
    "    def processTweets(self,list_of_tweets):\n",
    "        # The list of tweets is a list of dictionaries which should have the keys, \"text\" and \"label\"\n",
    "        processedTweets=[]\n",
    "        # This list will be a list of tuples. Each tuple is a tweet which is a list of words and its label\n",
    "        for tweet in list_of_tweets:\n",
    "            processedTweets.append((self._processTweet(tweet[\"text\"]),tweet[\"label\"]))\n",
    "        return processedTweets\n",
    " \n",
    "\n",
    "    def _processTweet(self,tweet):\n",
    "        # 1. Convert to lower case\n",
    "        tweet=tweet.lower()\n",
    "        # 2. Replace links with the word URL \n",
    "        tweet=re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','URL',tweet)     \n",
    "        # 3. Replace @username with \"AT_USER\"\n",
    "        tweet=re.sub('@[^\\s]+','AT_USER',tweet)\n",
    "        # 4. Replace #word with word \n",
    "        tweet=re.sub(r'#([^\\s]+)',r'\\1',tweet)\n",
    "        # You can do further cleanup as well if you like, replace \n",
    "        # repetitions of characters, for ex: change \"huuuuungry\" to \"hungry\"\n",
    "        # We'll leave that as an exercise for you on regular expressions\n",
    "        tweet=word_tokenize(tweet.decode('utf-8'))\n",
    "        # This tokenizes the tweet into a list of words \n",
    "        # Let's now return this list minus any stopwords \n",
    "        # Remove the line breakers and carriage returns. The string '\\n' represents newlines.\n",
    "        # And \\r represents carriage returns \n",
    "        return [word for word in tweet if word not in self._stopwords]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweetProcessor=PreProcessTweets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ppTrainingData=tweetProcessor.processTweets(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ppTrainingDataSVM=tweetProcessor.processTweets(trainingDataSVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk \n",
    "# Naive Bayes Classifier - We'll use NLTK's built in classifier to perform the classification\n",
    "\n",
    "# First build a vocabulary \n",
    "def buildVocabulary(ppTrainingData):\n",
    "    all_words=[]\n",
    "    for (words,sentiment) in ppTrainingData:\n",
    "        all_words.extend(words)\n",
    "    # This will give us a list in which all the words in all the tweets are present\n",
    "    # These have to be de-duped. Each word occurs in this list as many times as it \n",
    "    # appears in the corpus \n",
    "    wordlist=nltk.FreqDist(all_words)\n",
    "    # This will create a dictionary with each word and its frequency\n",
    "    word_features=wordlist.keys()\n",
    "    # This will return the unique list of words in the corpus \n",
    "    return word_features\n",
    "\n",
    "# NLTK has an apply_features function that takes in a user-defined function to extract features \n",
    "# from training data. We want to define our extract features function to take each tweet in \n",
    "# The training data and represent it with the presence or absence of a word in the vocabulary \n",
    "\n",
    "def extract_features(tweet):\n",
    "    tweet_words=set(tweet)\n",
    "    features={}\n",
    "    for word in word_features:\n",
    "        features['contains(%s)' % word]=(word in tweet_words)\n",
    "        # This will give us a dictionary , with keys like 'contains word1' and 'contains word2'\n",
    "        # and values as True or False \n",
    "    return features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now we can extract the features and train the classifier \n",
    "word_features = buildVocabulary(ppTrainingData)\n",
    "trainingFeatures=nltk.classify.apply_features(extract_features,ppTrainingData)\n",
    "# apply_features will take the extract_features function we defined above, and apply it it \n",
    "# each element of ppTrainingData. It automatically identifies that each of those elements \n",
    "# is actually a tuple , so it takes the first element of the tuple to be the text and \n",
    "# second element to be the label, and applies the function only on the text \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NBayesClassifier=nltk.NaiveBayesClassifier.train(trainingFeatures)\n",
    "# We now have a classifier that has been trained using Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Support Vector Machines \n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "import numpy as np \n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "\n",
    "# We have to change the form of the data slightly. SKLearn has a CountVectorizer object. \n",
    "# It will take in documents and directly return a term-document matrix with the frequencies of \n",
    "# a word in the document. It builds the vocabulary by itself. We will give the trainingData \n",
    "# and the labels separately to the SVM classifier and not as tuples. \n",
    "# Another thing to take care of, if you built the Naive Bayes for more than 2 classes, \n",
    "# SVM can only classify into 2 classes - it is a binary classifier. \n",
    "\n",
    "svmTrainingData=[' '.join(tweet[0]) for tweet in ppTrainingDataSVM]\n",
    "# Creates sentences out of the lists of words \n",
    "\n",
    "vectorizer=CountVectorizer(min_df=1)\n",
    "X=vectorizer.fit_transform(svmTrainingData).toarray()\n",
    "# We now have a term document matrix \n",
    "vocabulary=vectorizer.get_feature_names()\n",
    "\n",
    "# Now for the twist we are adding to SVM. We'll use sentiwordnet to add some weights to these \n",
    "# features \n",
    "\n",
    "swn_weights=[]\n",
    "\n",
    "for word in vocabulary:\n",
    "    try:\n",
    "        # Put this code in a try block as all the words may not be there in sentiwordnet (esp. Proper\n",
    "        # nouns). Look for the synsets of that word in sentiwordnet \n",
    "        synset=list(swn.senti_synsets(word))\n",
    "        # use the first synset only to compute the score, as this represents the most common \n",
    "        # usage of that word \n",
    "        common_meaning =synset[0]\n",
    "        # If the pos_Score is greater, use that as the weight, if neg_score is greater, use -neg_score\n",
    "        # as the weight \n",
    "        if common_meaning.pos_score()>common_meaning.neg_score():\n",
    "            weight=common_meaning.pos_score()\n",
    "        elif common_meaning.pos_score()<common_meaning.neg_score():\n",
    "            weight=-common_meaning.neg_score()\n",
    "        else: \n",
    "            weight=0\n",
    "    except: \n",
    "        weight=0\n",
    "    swn_weights.append(weight)\n",
    "\n",
    "\n",
    "# Let's now multiply each array in our original matrix with these weights \n",
    "# Initialize a list\n",
    "\n",
    "swn_X=[]\n",
    "for row in X: \n",
    "    swn_X.append(np.multiply(row,np.array(swn_weights)))\n",
    "# Convert the list to a numpy array \n",
    "swn_X=np.vstack(swn_X)\n",
    "\n",
    "\n",
    "# We have our documents ready. Let's get the labels ready too. \n",
    "# Lets map positive to 1 and negative to 2 so that everything is nicely represented as arrays \n",
    "labels_to_array={\"positive\":1,\"negative\":2}\n",
    "labels=[labels_to_array[tweet[1]] for tweet in ppTrainingDataSVM]\n",
    "y=np.array(labels)\n",
    "\n",
    "# Let's now build our SVM classifier \n",
    "from sklearn.svm import SVC \n",
    "SVMClassifier=SVC()\n",
    "SVMClassifier.fit(swn_X,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# if NBResultLabels.count('positive')>NBResultLabels.count('negative'):\n",
    "#    print \"NB Result Positive Sentiment\" + str(100*NBResultLabels.count('positive')/len(NBResultLabels))+\"%\"\n",
    "#else: \n",
    "#    print \"NB Result Negative Sentiment\" + str(100*NBResultLabels.count('negative')/len(NBResultLabels))+\"%\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#if SVMResultLabels.count(1)>SVMResultLabels.count(2):\n",
    "#    print \"SVM Result Positive Sentiment\" + str(100*SVMResultLabels.count(1)/len(SVMResultLabels))+\"%\"\n",
    "#else: \n",
    "#    print \"SVM Result Negative Sentiment\" + str(100*SVMResultLabels.count(2)/len(SVMResultLabels))+\"%\"\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "api = twitter.Api(consumer_key='Ecllhp6hUpjeFz14Sp84fOTmK',\n",
    "                 consumer_secret='wWMGD0db7pSbC39TuDFxQ6z84GaMPbck3T04w1JlSTKx19zopy',\n",
    "                 access_token_key='2455417567-YyIf76YAbN6sEydm4Z2EhXyuwyYtMCf6kTg70Ak',\n",
    "                 access_token_secret='E0kz81jDnLh2wICvXlAtPVhtwyA6e5JOoCljP3qYu94bO')\n",
    "\n",
    "# To see if this worked, use the command below, it will print out a bunch of details about your user account\n",
    "# and that's how you know you're all set to use the API\n",
    "print(api.VerifyCredentials())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi there! What are we searching for today?\"#Delta\"\n"
     ]
    }
   ],
   "source": [
    "def createTestData(search_string):\n",
    "    try:\n",
    "        tweets_fetched=api.GetSearch(search_string, count=100,lang='en')\n",
    "        # This will return a list with twitter.Status objects. These have attributes for \n",
    "        # text, hashtags etc of the tweet that you are fetching. \n",
    "        # The full documentation again, you can see by typing pydoc twitter.Status at the \n",
    "        # command prompt of your terminal \n",
    "        print \"Great! We fetched \"+str(len(tweets_fetched))+\" tweets with the term \"+search_string+\"!!\"\n",
    "        # We will fetch only the text for each of the tweets, and since these don't have labels yet, \n",
    "        # we will keep the label empty \n",
    "        return [{\"text\":status.text,\"label\":None} for status in tweets_fetched]\n",
    "    except:\n",
    "        print \"Sorry there was an error!\"\n",
    "        return None\n",
    "    \n",
    "search_string=input(\"Hi there! What are we searching for today?\")\n",
    "testData=createTestData(search_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get rid of the breaklines and carriage return\n",
    "for a in testData:\n",
    "    a['text']=a['text'].replace('\\n', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for a in testData:\n",
    "    a['text']=a['text'].replace('\\r', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ppTestData=tweetProcessor.processTweets(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First Naive Bayes \n",
    "NBResultLabels=[NBayesClassifier.classify(extract_features(tweet[0])) for tweet in ppTestData]\n",
    "\n",
    "\n",
    "\n",
    "# Now SVM \n",
    "SVMResultLabels=[]\n",
    "for tweet in ppTestData:\n",
    "    tweet_sentence=' '.join(tweet[0])\n",
    "    svmFeatures=np.multiply(vectorizer.transform([tweet_sentence]).toarray(),np.array(swn_weights))\n",
    "    SVMResultLabels.append(SVMClassifier.predict(svmFeatures)[0])\n",
    "    # predict() returns  a list of numpy arrays, get the first element of the first array \n",
    "    # there is only 1 element and array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Write the result to csvfile to be imported to Spotfire\n",
    "def WriteResultToCSV(tweetsData,ResultLabels,Searchterm, ResultLabelFile):\n",
    "    import csv\n",
    "    with open(ResultLabelFile,'wb') as csvfile:\n",
    "        linewriter=csv.writer(csvfile,delimiter=',',quotechar=\"\\\"\")\n",
    "        for row in range(0,len(tweetsData)):\n",
    "            try:\n",
    "                #linewriter.writerow(Searchterm,tweetsData[row]['text'], ResultLabels[row])\n",
    "                linewriter.writerow([Searchterm, ResultLabels[row],tweetsData[row]['text']])\n",
    "            except Exception:\n",
    "                print \"error\"\n",
    "    return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ResultLabelFile='C:/Users/songsu/Desktop/Spotfire_Sentiment Analysis/Result.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ResultLabelFileSVM='C:/Users/songsu/Desktop/Spotfire_Sentiment Analysis/ResultSVM.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Result=WriteResultToCSV(testData,NBResultLabels,search_string,ResultLabelFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ResultSVM=WriteResultToCSV(testData,SVMResultLabels,search_string,ResultLabelFileSVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
